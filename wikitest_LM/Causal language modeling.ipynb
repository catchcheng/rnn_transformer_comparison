{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010f2995-7899-43bf-b0a9-3a44083d8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c5bfc-cc43-49ec-af16-37f5f6134297",
   "metadata": {},
   "source": [
    "## Causal language modeling\n",
    "the model has to predict the next token in the sentence (so the labels are the same as the inputs shifted to the right). To make sure the model does not cheat, it gets an attention mask that will prevent it to access the tokens after token i when trying to predict the token i+1 in the sentence.\n",
    "## Masked language modeling\n",
    "the model has to predict some tokens that are masked in the input. It still has access to the whole sentence, so it can use the tokens before and after the tokens masked to predict their value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7cf26d-b55b-4dad-a6c9-c76f01fe2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ea9fbc-505d-44d8-8f95-a2ed506e5469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cfc721-7541-4bcb-a50b-037347a4ce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cougars are slender and agile members of the cat family . They are the fourth @-@ largest cat ; adults stand about 60 to 90 cm ( 24 to 35 in ) tall at the shoulders . Adult males are around 2 @.@ 4 m ( 7 @.@ 9 ft ) long nose @-@ to @-@ tail and females average 2 @.@ 05 m ( 6 @.@ 7 ft ) , with overall ranges between 1 @.@ 5 to 2 @.@ 75 m ( 4 @.@ 9 to 9 @.@ 0 ft ) nose to tail suggested for the species in general . Of this length , 63 to 95 cm ( 25 to 37 in ) is comprised by the tail . Males typically weigh 53 to 100 kg ( 115 to 220 lb ) , averaging 62 kg ( 137 lb ) . Females typically weigh between 29 and 64 kg ( 64 and 141 lb ) , averaging 42 kg ( 93 lb ) . Cougar size is smallest close to the equator , and larger towards the poles . The largest recorded cougar , shot in 1901 , weighed 105 @.@ 2 kg ( 232 lb ) ; claims of 125 @.@ 2 kg ( 276 lb ) and 118 kg ( 260 lb ) have been reported , though they were most likely exaggerated . On average , adult male cougars in British Columbia weigh 56 @.@ 7 kg ( 125 lb ) and adult females 45 @.@ 4 kg ( 100 lb ) , though several male cougars in British Columbia weighed between 86 @.@ 4 and 95 @.@ 5 kg ( 190 to 210 lb ) . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Citizens in the south were opposed to a centralised government , and to the decrees of its rule , which resulted in rebellion . Prior to the revolution France had been divided into provinces with local governments . In 1790 the government , the National Constituent Assembly , reorganised France into administrative departments in order to rebalance the uneven distribution of French wealth , which had been subject to feudalism under the monarchical Ancien Régime . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hobbs was born on May 8 , 1883 , in Bloomington , Nebraska , to John Alden Hobbs and Cora Bush Hobbs . Her family moved to Salt Lake City , Utah when she was six years old ; she lived there for 12 years , finishing high school . Her father then met with financial difficulties , and she moved to Oregon , settling in Hillsboro . There , she put her younger brother and sister through school , while studying stenography and working for a living . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roger Federer at the Davis Cup \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Major declines in populations have been observed from 1980 onward in Sweden , Finland , northern Russia ( Karelia ) and the Baltic States , and smaller declines in much of the rest of northern and central Europe . The bird has been adversely affected in these areas by intensive agriculture , and in several countries it has been red @-@ listed due to population declines of more than 50 % . Numbers dwindled in the United Kingdom by more than 80 % between 1966 and 2004 ; although populations in some areas such as Northern Ireland were stable or even increased , those in other areas , mainly England , declined even more sharply . The overall decline seems to be due to the low survival rate of young birds , which may be caused by changes in agricultural practices . The intensive farming methods used in northern Europe mean there is less pasture and meadow habitat available , and the supply of grassland invertebrates needed for the nestlings to thrive is correspondingly reduced . \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a49839-c43e-402f-b498-5df3d1c7706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model_checkpoint = \"distilgpt2\"\n",
    "# model_checkpoint = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_checkpoint = \"EleutherAI/pythia-160m\"\n",
    "# model_checkpoint = \"RWKV/rwkv-4-169m-pile\"\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f8624c9-e6a3-4ed8-b80d-5f0b4f35309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [426, 657, 1278, 90, 5182, 28289, 868, 6490, 426, 2490],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "    \n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ba339f-dc95-4d04-ae87-ee3bef66ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab43a43-05b3-4714-88a9-bf54bb7d1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d45d782-21cb-4c17-ac7d-c95157cc2978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 11:47:48.502220: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-11 11:47:48.503575: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-11 11:47:48.519457: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-11 11:47:48.519473: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-11 11:47:48.519487: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-11 11:47:48.523222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 11:47:48.983874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' time gameplay as its predecessors, the story runs parallel to the first game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e50d3c1-86d6-49ed-a4ba-502ef32600cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f798f068-bbbe-4714-a68c-18d7eb1049fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 229,376 || all params: 162,552,320 || trainable%: 0.1411090287730129\n"
     ]
    }
   ],
   "source": [
    "# Add peft\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType, PeftType, get_peft_model_state_dict, set_peft_model_state_dict, PromptEncoderConfig\n",
    "\n",
    "# ## Prefix-tuning\n",
    "# peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
    "\n",
    "## P-tuning\n",
    "peft_type = PeftType.P_TUNING\n",
    "# peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20, encoder_hidden_size=128)\n",
    "peft_config = PromptEncoderConfig(task_type=\"CAUSAL_LM\", num_virtual_tokens=20, encoder_hidden_size=128)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f47e908-3412-4f9c-8cd6-75f2d5579b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: transformers in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (4.39.0.dev0)\n",
      "Requirement already satisfied: tqdm in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (0.27.2)\n",
      "Requirement already satisfied: safetensors in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/cheng-ubuntu/.local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cheng-ubuntu/.local/lib/python3.11/site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/cheng-ubuntu/anaconda3/envs/6758hmk4/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.10.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f5800b-f57b-4f66-83cb-5ce5e480c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28dc0a0e-7a50-425f-8590-c7e3eb71ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# Zero shot evaluation\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e0a684-64bc-471d-8d2b-cbc82447023a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [244/244 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myimei-yang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cheng-ubuntu/Documents/ift6759/wandb/run-20240411_114801-mou2geb8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yimei-yang/huggingface/runs/mou2geb8' target=\"_blank\">earthy-oath-17</a></strong> to <a href='https://wandb.ai/yimei-yang/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yimei-yang/huggingface' target=\"_blank\">https://wandb.ai/yimei-yang/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yimei-yang/huggingface/runs/mou2geb8' target=\"_blank\">https://wandb.ai/yimei-yang/huggingface/runs/mou2geb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 87.91\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d78f5-d8dc-4783-affb-9bbfd7e742bc",
   "metadata": {},
   "source": [
    "Perplexity: 45.93 for pythia-70m-deduped\n",
    "\n",
    "Perplexity: 34.95 for pythia-160m-deduped, with pre-fix tuning: Perplexity: 2739.01, with p-tuning: Perplexity: 87.91\n",
    "\n",
    "Perplexity: 26.12 for RWKV/rwkv-4-169m-pile\n",
    "\n",
    "Perplexity: 61.54 for pythia-160m zero shot evaluation\n",
    "\n",
    "Perplexity: 51.08 for RWKV/rwkv-4-169m-pile zero shot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624439c1-6e8b-4d96-8a6a-298af0b7b257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6758hmk4",
   "language": "python",
   "name": "6758hmk4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
